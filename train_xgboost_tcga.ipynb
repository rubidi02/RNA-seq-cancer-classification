{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d528b1",
   "metadata": {},
   "source": [
    "# TCGA dataset exploratory data analysis\n",
    "\n",
    "The data is downloaded from TCGA database (https://portal.gdc.cancer.gov/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d5f3a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "AUGMENTATION = 0\n",
    "\n",
    "# Specify the path to the parent directory containing the folders to iterate through\n",
    "parent_dir = \"../data/data-tcga/samples\"\n",
    "sample_sheet_file = \"../data/data-tcga/gdc_sample_sheet.2023-05-20.tsv\"\n",
    "clinical_file = \"../data/data-tcga/clinical.tsv\"\n",
    "\n",
    "all_data_file_path = \"all_data.pkl\"\n",
    "all_labels_file_path = \"all_labels.pkl\"\n",
    "all_data_columns_file_path = \"all_data_columns.pkl\"\n",
    "ignored_tissues_file_path = \"ignored_tissues_list.pkl\"\n",
    "class_mapping_file_path = \"class_mapping.pkl\"\n",
    "encoder_file = f\"encoder_{AUGMENTATION}.pkl\"\n",
    "model_file_name = f\"xgboost-{AUGMENTATION}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34698e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class mapping\n",
    "class_mapping = {}\n",
    "# ignore unknown tissues\n",
    "ignored_tissues = []\n",
    "\n",
    "# load the files if they exist\n",
    "\n",
    "if os.path.exists(class_mapping_file_path):\n",
    "\n",
    "    # load the list of ignored tissues from a pickel file\n",
    "    with open(ignored_tissues_file_path, 'rb') as f: \n",
    "        ignored_tissues = pickle.load(f)\n",
    "\n",
    "    # load a dataframe with the labels of each sample from a pickel file\n",
    "    with open(class_mapping_file_path, 'rb') as f: \n",
    "        class_mapping = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c46f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tissue origin found: \"Middle lobe, lung\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "lung\n",
      "New tissue origin found: \"Sigmoid colon\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "colon\n",
      "New tissue origin found: \"Brain, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"Cerebrum\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"Head of pancreas\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "pancreas\n",
      "New tissue origin found: \"Kidney, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "kidney\n",
      "New tissue origin found: \"Occipital lobe\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"Breast, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "breast\n",
      "New tissue origin found: \"Skin, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "skin\n",
      "New tissue origin found: \"Liver\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "liver\n",
      "New tissue origin found: \"Prostate gland\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "prostate\n",
      "New tissue origin found: \"Cecum\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "colon\n",
      "New tissue origin found: \"Trigone of bladder\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "bladder\n",
      "New tissue origin found: \"Intrahepatic bile duct\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "liver\n",
      "New tissue origin found: \"Main bronchus\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "lung\n",
      "New tissue origin found: \"Not Reported\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "\n",
      "New tissue origin found: \"Temporal lobe\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"Frontal lobe\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"'--\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "\n",
      "New tissue origin found: \"Unknown\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "\n",
      "New tissue origin found: \"Pleura, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "lung\n",
      "New tissue origin found: \"Parietal lobe\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"Head, face or neck, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "\n",
      "New tissue origin found: \"Cerebellum, NOS\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "brain\n",
      "New tissue origin found: \"Ureteric orifice\". \n",
      "Enter the label for it or leave it empty to add it to the ignore list: \n",
      "bladder\n",
      "Tissue and counts extracted sucessfully!\n"
     ]
    }
   ],
   "source": [
    "# verify if the all_data_file_path exist or not\n",
    "if not os.path.exists(all_data_file_path):\n",
    "    \n",
    "    # read sample sheet file and clinical file into the memory\n",
    "    sample_sheet = pd.read_csv(sample_sheet_file, delimiter=\"\\t\")\n",
    "    clinical_sheet = pd.read_csv(clinical_file, delimiter=\"\\t\")\n",
    "    \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate through all folders in the parent directory\n",
    "    for folder in os.listdir(parent_dir):\n",
    "        # Create the full path to the folder\n",
    "        folder_path = os.path.join(parent_dir, folder)\n",
    "        # Check if the item in the directory is a folder\n",
    "        if os.path.isdir(folder_path):\n",
    "\n",
    "            # try to extract the cancer tissue first, if possible\n",
    "            row = sample_sheet[ sample_sheet['File ID'] == folder ]\n",
    "            \n",
    "            # confirm that there is at most 1 row extracted for the current folder\n",
    "            assert(row.shape[0] < 2)\n",
    "\n",
    "            # access the value stored in Case ID\n",
    "            case_id = row['Case ID'].values[0]\n",
    "\n",
    "            case_id_clinical = clinical_sheet[clinical_sheet['case_submitter_id'] == case_id]\n",
    "            if len(case_id_clinical['tissue_or_organ_of_origin'].values) == 0:\n",
    "                continue\n",
    "\n",
    "            tissue_organ_origin = case_id_clinical['tissue_or_organ_of_origin'].values[0]\n",
    "\n",
    "            # skipping the labels stored in ignored_tissue list:\n",
    "            if tissue_organ_origin in ignored_tissues:\n",
    "                continue\n",
    "\n",
    "            # storing the labels in a dictionary\n",
    "            if tissue_organ_origin not in class_mapping:\n",
    "                new_label = None\n",
    "                # try to find an already seen label in tissue_organ_origin automatically\n",
    "                for label in all_labels:\n",
    "                    # converting label from dataframe to string\n",
    "                    label = label[0][0]\n",
    "                    if label in tissue_organ_origin.lower():\n",
    "                        new_label = label\n",
    "                        break\n",
    "                # manual user input required to determine the label\n",
    "                if new_label is None:\n",
    "                    print(f'New tissue origin found: \\\"{tissue_organ_origin}\\\". \\nEnter the label for it or leave it empty to add it to the ignore list: ')\n",
    "                    new_label = input()\n",
    "\n",
    "                    if new_label == \"\":\n",
    "                        ignored_tissues.append(tissue_organ_origin)\n",
    "                        continue\n",
    "                class_mapping[tissue_organ_origin] = new_label\n",
    "\n",
    "            # tissue extracted successfully!\n",
    "            all_labels.append(pd.DataFrame([class_mapping[tissue_organ_origin]]))\n",
    "\n",
    "\n",
    "            # extract the gene counts\n",
    "\n",
    "            # Get a list of all .tsv files in the folder\n",
    "            tsv_files = [f for f in os.listdir(folder_path) if f.endswith('.tsv')]\n",
    "            # If there is exactly one .tsv file in the folder, proceed\n",
    "            if len(tsv_files) == 1:\n",
    "                # Create the full path to the .tsv file\n",
    "                tsv_path = os.path.join(folder_path, tsv_files[0])\n",
    "                \n",
    "                # Open the .tsv file and count the number of lines starting with \"ENSG\"\n",
    "                columns = []\n",
    "                counts = []\n",
    "                with open(tsv_path, 'r') as tsv_file:\n",
    "                    for line in tsv_file:\n",
    "                        if line.startswith(\"ENSG\"):\n",
    "\n",
    "                            split_line = line.split('\\t')\n",
    "\n",
    "                            gene_name = split_line[1]\n",
    "                            tpm = split_line[6]\n",
    "                            columns.append(gene_name)\n",
    "                            counts.append(tpm)\n",
    "\n",
    "                row_df = pd.DataFrame(counts, dtype=np.float32)\n",
    "                all_data.append(row_df)\n",
    "\n",
    "    print(\"Tissue and counts extracted sucessfully!\")\n",
    "\n",
    "    all_data = pd.concat(all_data, axis=1, ignore_index=True).transpose()\n",
    "    all_data.columns = columns\n",
    "    all_labels = pd.concat(all_labels, axis=0, ignore_index=True)\n",
    "\n",
    "    # store the count matrices and labels into pickle files\n",
    "    with open(all_data_file_path, 'wb') as all_data_pckl:\n",
    "        pickle.dump(all_data, all_data_pckl)\n",
    "    with open(all_labels_file_path, 'wb') as all_labels_pckl:\n",
    "        pickle.dump(all_labels, all_labels_pckl)\n",
    "    \n",
    "\n",
    "else: # .pkl file exists, load everything from it to skip processing\n",
    "\n",
    "    with open(all_data_file_path, 'rb') as all_data_pckl:\n",
    "        all_data = pickle.load(all_data_pckl)\n",
    "    with open(all_labels_file_path, 'rb') as all_labels_pckl:\n",
    "        all_labels = pickle.load(all_labels_pckl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60c1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the files if they do not exist\n",
    "if not os.path.exists(class_mapping_file_path):\n",
    "    \n",
    "    # save the list of ignored tissues in a pickel file\n",
    "    with open(ignored_tissues_file_path, 'wb') as f: \n",
    "        pickle.dump(ignored_tissues, f)\n",
    "\n",
    "    # save a dataframe with the labels of each sample in a pickel file\n",
    "    with open(class_mapping_file_path, 'wb') as f: \n",
    "        pickle.dump(class_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02113cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data_columns:  Index(['TSPAN6', 'TNMD', 'DPM1', 'SCYL3', 'C1orf112', 'FGR', 'CFH', 'FUCA2',\n",
      "       'GCLC', 'NFYA',\n",
      "       ...\n",
      "       'AL451106.1', 'AC092910.4', 'AC073611.1', 'AC136977.1', 'AC078856.1',\n",
      "       'AC008763.4', 'AL592295.6', 'AC006486.3', 'AL391628.1', 'AP006621.6'],\n",
      "      dtype='object', length=60660)\n",
      "Total gene count: 60660\n",
      "Unique gene count: 59427\n"
     ]
    }
   ],
   "source": [
    "# save the all_data columns in a list\n",
    "all_data_columns = all_data.columns\n",
    "print(\"all_data_columns: \", all_data_columns)\n",
    "print(f'Total gene count: {len(all_data_columns)}')\n",
    "unique_columns = list(set(all_data_columns))\n",
    "print(f'Unique gene count: {len(unique_columns)}')\n",
    "\n",
    "# save the columns to use it later to organize the columns of different dataset\n",
    "with open(all_data_columns_file_path, 'wb') as all_data_columns_pckl: \n",
    "    pickle.dump(unique_columns, all_data_columns_pckl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf10dbd",
   "metadata": {},
   "source": [
    "The column names are genes as expected, the problem now is that we have duplicated columns and xgboost will complain. The solution will be adding the counts of columns with the same gene name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81dca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gene count: 59427\n",
      "Unique gene count: 59427\n"
     ]
    }
   ],
   "source": [
    "all_data_unique = all_data.groupby(all_data.columns, axis=1).sum()\n",
    "print(f'Total gene count: {len(all_data_unique.columns)}')\n",
    "unique_columns = len(set(all_data_unique.columns))\n",
    "print(f'Unique gene count: {unique_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0056846",
   "metadata": {},
   "source": [
    "Now we are ready to train, since all columns have a unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bec991",
   "metadata": {},
   "source": [
    "**Training xgBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07bd8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc085767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "random_percentage = AUGMENTATION / 100.0\n",
    "\n",
    "all_data_augmented = all_data_unique\n",
    "all_labels_augmented = all_labels\n",
    "\n",
    "if random_percentage != 0:\n",
    "    # set the seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # copy all_data dataset\n",
    "    all_data_copy = all_data_unique.copy()\n",
    "    \n",
    "    # number of columns that will be set to zero\n",
    "    random_count = int(len(all_data_copy.columns) * random_percentage)\n",
    "    print(\"Number of columns that will be set to zero: \", random_count)\n",
    "    \n",
    "    # list of column indixes (picked randomly) that will be set to zero\n",
    "    random_index_list = np.random.randint(0, len(all_data_copy), random_count)\n",
    "    \n",
    "    # set the selected columns from all_data_copy to zero\n",
    "    all_data_copy[ all_data_copy.columns[ random_index_list ] ] = 0\n",
    "\n",
    "    # all_data augmented\n",
    "\n",
    "    all_data_augmented = pd.concat([all_data_unique, all_data_copy], axis = 0)\n",
    "    print(\"Shape of all_data_augmented dataset: \", all_data_augmented.shape)\n",
    "\n",
    "    all_labels_augmented = pd.concat([all_labels, all_labels], axis = 0)\n",
    "    print(\"Shape of all_labels_augmented dataset: \", all_labels_augmented.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47799d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "Training the model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "all_labels_encoded = label_encoder.fit_transform(all_labels_augmented[0].to_list())\n",
    "\n",
    "#Split dataset into training and validation sets\n",
    "all_data_train, all_data_validation, all_labels_encoded_train, all_labels_encoded_validation = \\\n",
    "    train_test_split(all_data_augmented, all_labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "duplicated = all_data_train.columns[all_data_train.columns.duplicated()]\n",
    "print(duplicated)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "\n",
    "model_train : xgb.XGBClassifier = xgb.XGBClassifier()\n",
    "model_train.fit(all_data_train, all_labels_encoded_train)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae561acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the label encoder for later use\n",
    "with open(encoder_file, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebad3473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying accuracy...\n",
      "Accuracy of the train: 100.0000%\n",
      "Accuracy of the validation: 98.9619%\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying accuracy...\")\n",
    "# verify accuracy of the train data\n",
    "pred_train = model_train.predict(all_data_train)\n",
    "\n",
    "accuracy = accuracy_score(pred_train, all_labels_encoded_train)\n",
    "print(\"Accuracy of the train: %.4f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# verify accuracy of the validation data\n",
    "pred_validation = model_train.predict(all_data_validation)\n",
    "\n",
    "accuracy = accuracy_score(pred_validation, all_labels_encoded_validation)\n",
    "print(\"Accuracy of the validation: %.4f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451be13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model for later use\n",
    "with open(model_file_name, 'wb') as file_model:\n",
    "    pickle.dump(model_train, file_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca14d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
